{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning Lab 4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yngvib/DeepLearningCourse/blob/master/Deep_Learning_Lab_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "w9uoEkc2_Zo_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lab 04\n",
        "\n",
        "In this lab we are going to do sentiment analysis on text-based movie reviews taken from IMDB (learn whether a review is positive or negative).\n",
        "\n",
        "We will be following the tutorial at:\n",
        "\n",
        "\n",
        "*   https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "7yHQx-yPkn8a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# In here we are going to import the necesasry libraries (packages)\n",
        "# and do some bookkeeping.\n",
        "import numpy\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "# Fix the random seed for reproducibility.\n",
        "numpy.random.seed(42)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oDDIQ65X_6jc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Next we are going to do some book-keepinng, fetch the IMDB dataset, and split\n",
        "# it up into training and testing sequences.\n",
        "# For further imformation about the dataset see e.g.:\n",
        "#   https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification\n",
        "\n",
        "# Load the dataset, but only keep the top n words (zero the rest)\n",
        "top_words = 5000\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
        "print( X_test );\n",
        "print( y_test );\n",
        "\n",
        "# Next we make the sequences all be of the same length (so we can use them as\n",
        "# input into our RNN); some are truncated while others are padded.\n",
        "max_review_length = 500\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
        "print( X_test );\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W0GJmxC4lpZw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Next we create our model. Note, that we use word embeddings as our input layer.\n",
        "# To learn more about word embeddings, see e.g.: \n",
        "# - https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
        "# - https://keras.io/layers/embeddings/#embedding\n",
        "embedding_vecor_length = 32\n",
        "num_epochs = 1\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "model.fit(X_train, y_train, epochs=num_epochs, batch_size=64)\n",
        "\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f5-dmqqEBKaN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Improving model's performance \n",
        "Now, we are going to see if we can improve the performance of the model.\n",
        "We start by adding dropout layers (to minimize the risk of overfitting)\n"
      ]
    },
    {
      "metadata": {
        "id": "qWWYjMOVBoig",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Model, but now with dropout layers.\n",
        "from keras.layers import Dropout\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "utRPihw4hUBR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Model, but now with dropout as arguments to LSTM layers (recurrent connection dropout as well)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i6eWtYdAiL-M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Add convoluational layer as well (on the 1D embedding representation)\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}